Sys.getlocale()
assignment1 <- c(15,77,56,12,45,36,7,99,82,63)
mean(assignment1)
median(assignment1)
sd(assignment1)
install.packages(tidyverse)
install.packages("tidyverse")
install.packages("dplyr")
2^1.5
2^2
install.packages("pdftools")
install.packages(tm)
install.packages("tm")
install.packages("Rpoppler")
Sys.getenv()
Sys.getlocale()
Sys.setlocale("LC_ALL","English")
Sys.setlocale("LC_ALL","English")
Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
Sys.setenv(LANG = "en_US.UTF-8")
Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
Sys.getlocale()
?Sys.setlocale
Sys.getlocale(category = "LC_ALL")
Sys.setlocale(category = "LC_ALL", locale = "")
Sys.setlocale(category = "LC_ALL", locale = "en")
install.koRpus.lang("en")
# load the package
library(koRpus.lang.en)
install.koRpus.lang("en")
library(koRpus.lang.en)
Sys.setlocale(category = "LC_ALL", locale = "en")
install.packages("tm")
install.packages("qdap")
install.packages("wordcloud")
install.packages("wordcloud2")
install.packages("RColorBrewer")
######## set up all the packages
library(tokenizers)
library(tm)
library(rJava)
library(openNLP)
library(coreNLP)
require("NLP")
## must run this one first
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
####### load data
options(java.parameters = "- Xmx1024m")
load("workspaces/CSR_documents_30samples.RData")
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
## must run this one first
#options(java.parameters = "- Xmx1024m")
library(XLConnect)
## must run this one first
#options(java.parameters = "- Xmx1024m")
install.packages("XLConnect")
library(XLConnect)
initCoreNLP(type = "english", parameterFile = NULL,
mem = "2g")
## must run this one first
#options(java.parameters = "- Xmx1024m")
#install.packages("XLConnect")
#library(XLConnect)
initCoreNLP(type = "english", parameterFile = NULL,
mem = "8g")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
## generate data
x1<- rnrom(200)
library(tidyverse)
## generate data
x1<- rnrom(200)
## generate data
x1<- rnorm(200)
x2<- rnorm(200)
??rnorm
y<-  -2+1.5*x1-0.8*x2+rnorm(200)
dat<- data.frame(y,x1,x2)
## random split to training and testing
index<- sample(nrow(dat),0.8*nrow(dat))
train<- dat[index,]
test<- dat[-index,]
#GoHawks#2020
install.packages("odbc")
install.packages("Sleuth2")
library(Sleuth2)
head(ex1029)
model = lm(wage~Education+Experience+Black+SMSA+Region, data = ex1029)
model = lm(Wage~Education+Experience+Black+SMSA+Region, data = ex1029)
summary(model)
install.packages("alr3")
library(alr3)
fuel2001 <- fuel2001
View(fuel2001)
m1 = lm(fuel2001$FuelC~., data = fuel2001)
summary(m1)
par(mfrow = c(2, 2))
plot(m1)
remove_spots <- which(row.names(fuel2001) %in% c("FL","TX","NY"))
no_tx_fl_ny <- fuel2001[-remove_spots,]
new_model <- lm(FuelC ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(new_model)
summary(m1)
attach(fuel2001)
m1 = lm(fuel2001$FuelC~Tax + Drivers + Income + log(Miles,10), data = fuel2001)
summary(m1)
par(mfrow = c(2, 2))
plot(m1)
hist(fuel2001$FuelC)
m2 <- lm(log(FuelC) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
m2 <- lm(log(FuelC,10) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
m2 <- lm(log(FuelC) ~ Tax + Drivers + Income + log(Miles,10), no_tx_fl_ny)
summary(m2)
exp(coefficients(m2))
plot(m2)
par(mfrow = c(2, 2))
plot(m2)
load("D:/1Github/CSRtm/workspaces/Specificity_Scores.RData")
house_data <- read.csv("http://www.lock5stat.com/datasets/HomesForSale.csv")
log_price <- log(house_data$Price, 10)
m1<- lm(log_price~Size+Beds, data = house_data)
summary(m1)
m2<- lm(log_price~Size+Beds+Baths, data = house_data)
summary(m2)
m3<- lm(log_price~Size+Beds+Baths+State, data = house_data)
summary(m1)
summary(m3)
anova(m1,m2)
anova(m2,m3)
AIC(m1,m2,m3)
help(state.x77)
state_data <- data.frame(state.x77)
state_data
pairs(state_data)
no_alaska <- state_data[-2,]
inter_model <- lm(Life.Exp ~1,no_alaska)
forward_model <- step(inter_model,direction = "forward",scope = (~Population+Income+Illiteracy+Murder+HS.Grad+Frost+Area))
final<-lm(Life.Exp ~ Murder + HS.Grad + Frost + Population, data = no_alaska)
summary(final)
setwd("D:/1Github/CSRtm")
knitr::opts_chunk$set(echo = TRUE)
load("workspaces/CSR_documents_30samples.RData")
setwd("D:/1Github/CSRtm")
load("workspaces/CSR_documents_30samples.RData")
########load original data sample
load("workspaces/CSR_documents_30samples.RData")
text_stack_sample<-text_stack_sample[1:10,]
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
library(tokenizers)
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
library(tokenizers)
library(tm)
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
library(koRpus.lang.en)
library(tokenizers)
library(tm)
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
library(koRpus.lang.en)
library(tokenizers)
library(tm)
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
text_stack_sample$industry
load("workspaces/CSR_documents_30samples.RData")
knitr::opts_chunk$set(echo = TRUE)
load("workspaces/CSR_documents_30samples.RData")
library(koRpus.lang.en)
library(tokenizers)
library(tm)
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
ngram <- list(length=length(t))
for(i in 1:length(t))
{
print(i)
ngram[[i]] = list(length = length(t[[i]]))
for(j in 1:length(t[[i]]))
{
try(
if(t[[i]][[j]] != "")
{
ngram[[i]][[j]] = tokenize_ngrams(t[[i]][[j]],n=4)
}
)
}
}
list_tetragrams = list(length(nrow(text_stack_sample)))
for(row in 1:nrow(text_stack_sample))
{
temp  = unlist(ngram[row])
temp = as.data.frame(table(temp))
list_tetragrams[[row]] = temp$temp
}
Fngram<- list(unlist(unlist(list_tetragrams)))
library(tidyverse)
N_table<-as.data.frame(table(Fngram))
N_table2 = N_table%>%
arrange(desc(Freq))%>%
mutate(prop=Freq/nrow(text_stack_sample)) %>% filter(prop>0.3 & prop<=0.75)
N_table2
N_table2
##  NWoS stands for Number of Words of each Sentence
for (i in 1:nrow(text_stack_sample)){
text_stack_sample$NWoS[[i]] <- lapply(t[[i]],function(x) str_count(x,'\\w+'))
}
## Num of tetragram in each sentence
sen_list<- list()
system.time(
for (i in 459:length(t)){
print(i)
sent_tetragram_count_list = list()
for(sent in 1:length(t[[i]]))
{
temp = 0
for (j in 1:nrow(N_table2)){
ngrams = na.omit(ngram[[i]][[sent]])
if(isTRUE(any(unlist(map(ngrams,str_detect,as.character(N_table2[j,1]))))))
{
temp = temp + 1
}
}
sent_tetragram_count_list[[sent]] = temp
}
sen_list[[i]] = sent_tetragram_count_list
}
)
##  NWoS stands for Number of Words of each Sentence
for (i in 1:nrow(text_stack_sample)){
text_stack_sample$NWoS[[i]] <- lapply(t[[i]],function(x) str_count(x,'\\w+'))
}
## Num of tetragram in each sentence
sen_list<- list()
system.time(
for (i in 1:length(t)){
print(i)
sent_tetragram_count_list = list()
for(sent in 1:length(t[[i]]))
{
temp = 0
for (j in 1:nrow(N_table2)){
ngrams = na.omit(ngram[[i]][[sent]])
if(isTRUE(any(unlist(map(ngrams,str_detect,as.character(N_table2[j,1]))))))
{
temp = temp + 1
}
}
sent_tetragram_count_list[[sent]] = temp
}
sen_list[[i]] = sent_tetragram_count_list
}
)
library(stringr)
## Get the length of each document
text_stack_sample$Length<-str_count(text_stack_sample[,1], '\\w+')
## Final Calculation
for (i in 1:nrow(text_stack_sample)){
temp = 0
for (sent in 1:length(text_stack_sample$NWoS[[i]])){
if (sen_list[[i]][[sent]] != 0){
temp = temp+text_stack_sample$NWoS[[i]][[sent]]
}
}
text_stack_sample$BoilerPlate[i] = temp / text_stack_sample$Length[i]
}
text_stack_sample$BoilerPlate
??tokenize_sentences
