install.packages("MASS")
### R file test
Sys.Date()
### R file test
Sys.Date()
qt(0.9,15)
####if the population sd is known as 2
k_lb <- 1002*3/sqrt(16)
####if the population sd is known as 2
k_lb <- 10-2*3/sqrt(16)
k_ub <- 10+2*3/sqrt(16)
a_k <- c(k_lb, k_ub)
a_k
####if we do not know population sd, and test at 95% confidence
tstar<- qt(0.975,15)
uk_lb <- 10-tstar*3/sqrt(16)
uk_up <- 10+tstar*3/sqrt(16)
uk_ub <- 10+tstar*3/sqrt(16)
ak <- c(uk_lb, uk_ub)
a_uk <- c(uk_lb, uk_ub)
a_uk
rm(ak)
rm(uk_up)
tstar <- qt(.975,15)
p_lb <- 10 - tstar*3*(1+1/sqrt(16))
p_ub <- 10 + tstar*3*(1+1/sqrt(16))
pred<-c(p_lb,p_ub)
pred
a_uk
pred<-c(p_lb,p_ub)
pred
pred<-c(p_lb, p_ub)
pred
pred<-c(p_lb, p_ub)
pred
rm(p_lb)
rm(p_ub)
p_lb <- 10 - tstar*3*(1+1/sqrt(16))
p_ub <- 10 + tstar*3*(1+1/sqrt(16))
pred <- c(p_lb, p_ub)
pred
setwd("J:/Github/Karthik/CSRtm")
############# load packages
library(koRpus.lang.en)
library(stringr)
library(tokenizers)
library(tidyverse)
library(tm)
library(rJava)
require(NLP)
setwd("J:/Github/Karthik/CSRtm")
############# load packages
library(koRpus.lang.en)
library(stringr)
library(tokenizers)
library(tidyverse)
library(tm)
library(rJava)
install.packages("XML")
library(XML)
library(coreNLP)
downloadCoreNLP()
downloadCoreNLP(type = c("base","english"))
downloadCoreNLP(type = c("base","english"))
downloadCoreNLP(type = "base")
downloadCoreNLP(type = "english")
initCoreNLP()
library(openNLP)
##################################
??openNLP
##################################
s <- "This is a sentence. This another---but with dash-like structures, and some commas.
Maybe another with question marks? Sure!"
sentDetect(s, language = "en")
??sentDetect
##################################
rm(s)
require("NLP")
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
setwd("J:/Github/Karthik/CSRtm")
##################### 2. Redundancy
####### % of 10-grams that occur more than once in each document
load("workspaces/CSR_documents_30samples.RData")
t<-list(length=nrow(text_stack_sample))
for(row in 1:nrow(text_stack_sample))
{
print(row)
if (text_stack_sample[row,1] != "")
{
t[[row]] =unlist(tokenize_sentences(removeNumbers(text_stack_sample[row,1])))
}
}
ngram<- NULL
for(i in 1:length(t))
{
print(i)
ngram[[i]] = list(length = length(t[[i]]))
for(j in 1:length(t[[i]]))
{
if(t[[i]][[j]] != "")
{
ngram[[i]][[j]] = tokenize_ngrams(t[[i]][[j]],n=10)
}
}
}
TenGram <- lapply(ngram,unlist)
TenGram <- lapply(TenGram, table)
TenGram <- lapply(TenGram, as.data.frame)
for (i in 1:length(TenGram)){
text_stack_sample$Redundancy[i]<-sum(TenGram[i][[1]]$Freq>=2)/sum(TenGram[i][[1]]$Freq)
}
text_stack_sample$Redundancy
TenGram
TenGram[1][[1]]
TenGram[1]
for (i in 1:length(TenGram)){
text_stack_sample$Redundancy[i]<-sum(TenGram[i]$Freq>=2)/sum(TenGram[i]$Freq)
}
text_stack_sample$Redundancy
for (i in 1:length(TenGram)){
text_stack_sample$Redundancy[i]<-sum(TenGram[i][[1]]$Freq>=2)/sum(TenGram[i][[1]]$Freq)
}
text_stack_sample$Redundancy
##################################
??cleanNLP
##################################
??CleanNLP
##################################
install.packages("cleanNLP")
library(cleanNLP)
load("J:/Github/Karthik/CSRtm/workspaces/CSR_documents_30samples.RData")
names(text_stack_sample)
head(text_stack_sample,1)
nrow(text_stack_sample)
