install.packages("MASS")
### R file test
Sys.Date()
### R file test
Sys.Date()
qt(0.9,15)
####if the population sd is known as 2
k_lb <- 1002*3/sqrt(16)
####if the population sd is known as 2
k_lb <- 10-2*3/sqrt(16)
k_ub <- 10+2*3/sqrt(16)
a_k <- c(k_lb, k_ub)
a_k
####if we do not know population sd, and test at 95% confidence
tstar<- qt(0.975,15)
uk_lb <- 10-tstar*3/sqrt(16)
uk_up <- 10+tstar*3/sqrt(16)
uk_ub <- 10+tstar*3/sqrt(16)
ak <- c(uk_lb, uk_ub)
a_uk <- c(uk_lb, uk_ub)
a_uk
rm(ak)
rm(uk_up)
tstar <- qt(.975,15)
p_lb <- 10 - tstar*3*(1+1/sqrt(16))
p_ub <- 10 + tstar*3*(1+1/sqrt(16))
pred<-c(p_lb,p_ub)
pred
a_uk
pred<-c(p_lb,p_ub)
pred
pred<-c(p_lb, p_ub)
pred
pred<-c(p_lb, p_ub)
pred
rm(p_lb)
rm(p_ub)
p_lb <- 10 - tstar*3*(1+1/sqrt(16))
p_ub <- 10 + tstar*3*(1+1/sqrt(16))
pred <- c(p_lb, p_ub)
pred
setwd("J:/Github/Karthik/CSRtm")
################################## load packages for NLP
#library(reticulate)
#use_python("C:/ProgramData/Microsoft/Windows/Start Menu/Programs/Python 3.8")
#install.packages("spacyr")
library(spacyr)
spacy_initialize(model = 'en_core_web_sm')
##### load files
load("workspaces/CSR_documents_30samples.RData")
#### parse the first document
document <- spacy_parse(text_stack_sample[1,1])
names(document)
document_entity<- entity_extract(document, type = "all")
head(document_entity,4)
###############extract entity for all the documents
document<- lappaly(text_stack_sample[,1],spacy_parse)
###############extract entity for all the documents
document<- lapply(text_stack_sample[,1],spacy_parse)
warnings()
document[[2]]
unique(document[[2]][,7])
document_entity<- sapply(document, function(x) entity_extract(,type = "all"))
document_entity<- lapply(document, function(x) entity_extract(,type = "all"))
document_entity<- lapply(document, entity_extract(,type = "all"))
#### parse the first document
document <- spacy_parse(text_stack_sample[1,1])
document_entity <- spacy_extract_entity(document, output = data.frame, type = "all")
document_entity <- spacy_extract_entity(document, output = "data.frame", type = "all")
document_entity <- spacy_extract_entity(document, type = "all")
document<-NULL
for (i in 1:nrow(text_stack_sample)){
print(i)
if(text_stack_sample[,1] != ""){
document[[i]]<-spacy_parse(text_stack_sample[i,1])
}
}
warnings()
###############extract entity for all the documents
document<- lapply(text_stack_sample[,1],spacy_parse) #it will take a while
length(document)
nrow(document)
document_entity<- NULL
for (i in 1:length(document)){
print(i)
if (document[[i]] != ""){
document_entity[[i]]<- entity_extract(document[[i]], type = "all")
}
}
warnings()
document_entity<- NULL
for (i in 1:length(document)){
print(i)
document_entity[[i]]<- entity_extract(document[[i]], type = "all")
}
################################### Add length
text_stack_sample$Length<-str_count(text_stack_sample[,1], '\\w+')
############# load basic packages
library(stringr)
################################### Add length
text_stack_sample$Length<-str_count(text_stack_sample[,1], '\\w+')
text_stack_sample$Length
################################## Add Entity Counts
text_stack_sample$EntityCount <- lapply(document_entity, length)
text_stack_sample$EntityCount
################################## Add Entity Counts
text_stack_sample$EntityCount <- lapply(document_entity, nrow)
text_stack_sample$EntityCount
#################################  calculate the specificity
text_stack_sample$Specificity <- text_stack_sample$EntityCount / text_stack_sample$Length
text_stack_sample$EntityCount
unlist(text_stack_sample$EntityCount)
text_stack_sample$Length
#################################  calculate the specificity
text_stack_sample$Specificity <- unlist(text_stack_sample$EntityCount) / text_stack_sample$Length
text_stack_sample$Specificity
save(text_stack_sample, file = "workspaces/Specificity.RData")
text_stack_sample$RelaPre
load("J:/Github/Karthik/CSRtm/workspaces/Relative_prevalece.RData")
text_stack_sample$RelaPre
load("J:/Github/Karthik/CSRtm/workspaces/Specificity.RData")
text_stack_sample$Specificity
